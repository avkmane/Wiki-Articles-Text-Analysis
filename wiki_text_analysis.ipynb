{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dd9a1b-7b25-4fe3-a9f6-45151c7b8ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'wiki-articles.json'\n",
    "\n",
    "# Utility function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator).lower()\n",
    "\n",
    "# Load dataset and preprocess articles\n",
    "texts = []\n",
    "titles = []\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        article_data = json.loads(line)\n",
    "        texts.append(preprocess_text(article_data['text']))\n",
    "        titles.append(article_data['title'])\n",
    "\n",
    "# === Question 1: Distribution of article lengths with enhanced analysis ===\n",
    "article_lengths = [len(text) for text in texts]\n",
    "\n",
    "# Compute statistical measures\n",
    "mean_length = np.mean(article_lengths)\n",
    "median_length = np.median(article_lengths)\n",
    "max_length = np.max(article_lengths)\n",
    "min_length = np.min(article_lengths)\n",
    "std_length = np.std(article_lengths)\n",
    "\n",
    "print(f\"Mean Length: {mean_length}\")\n",
    "print(f\"Median Length: {median_length}\")\n",
    "print(f\"Max Length: {max_length}\")\n",
    "print(f\"Min Length: {min_length}\")\n",
    "print(f\"Standard Deviation: {std_length}\")\n",
    "\n",
    "# Plot histogram and density plot\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.histplot(article_lengths, bins=50, kde=True, color='blue')\n",
    "plt.title(\"Distribution of Article Lengths\")\n",
    "plt.xlabel(\"Length of Articles (characters)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.axvline(mean_length, color='red', linestyle='--', label=f'Mean: {mean_length:.2f}')\n",
    "plt.axvline(median_length, color='green', linestyle='--', label=f'Median: {median_length:.2f}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# === Question 2: Most frequent words with TF-IDF and word clouds ===\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=50)\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "tfidf_scores = np.asarray(tfidf_matrix.sum(axis=0)).flatten()\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Visualize the most important keywords\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=feature_names, y=tfidf_scores, palette=\"viridis\")\n",
    "plt.title(\"Top 50 Keywords by TF-IDF Score\")\n",
    "plt.xlabel(\"Keywords\")\n",
    "plt.ylabel(\"TF-IDF Score\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(\n",
    "    dict(zip(feature_names, tfidf_scores))\n",
    ")\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud of Important Keywords\")\n",
    "plt.show()\n",
    "\n",
    "# === Question 3: Analyze rare characters ('x', 'z', 'j', 'q') ===\n",
    "rare_chars = set(\"xzjq\")\n",
    "char_counts = Counter()\n",
    "article_with_all_rare_chars = 0\n",
    "\n",
    "for text in texts:\n",
    "    if all(char in text for char in rare_chars):\n",
    "        article_with_all_rare_chars += 1\n",
    "    for char in rare_chars:\n",
    "        char_counts[char] += text.count(char)\n",
    "\n",
    "print(f\"Number of articles containing all rare characters: {article_with_all_rare_chars}\")\n",
    "\n",
    "# Bar plot for rare character frequency\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=list(char_counts.keys()), y=list(char_counts.values()), palette=\"magma\")\n",
    "plt.title(\"Frequency of Rare Characters ('x', 'z', 'j', 'q')\")\n",
    "plt.xlabel(\"Characters\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# === Question 4: Advanced Topic Modeling ===\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(tfidf_matrix)\n",
    "\n",
    "# Extract topics and their words\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "topic_words = []\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    topic_words.append([feature_names[i] for i in topic.argsort()[:-11:-1]])\n",
    "    print(f\"Topic {idx + 1}: {topic_words[-1]}\")\n",
    "\n",
    "# Visualize topic importance\n",
    "topic_weights = lda.components_.sum(axis=1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=[\"Topic \" + str(i + 1) for i in range(5)], y=topic_weights, palette=\"coolwarm\")\n",
    "plt.title(\"Topic Importance\")\n",
    "plt.xlabel(\"Topics\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.show()\n",
    "\n",
    "# === Question 5: Sentiment Analysis with Additional Metrics ===\n",
    "polarities = []\n",
    "subjectivities = []\n",
    "positive_count = 0\n",
    "negative_count = 0\n",
    "neutral_count = 0\n",
    "\n",
    "for text in texts:\n",
    "    blob = TextBlob(text)\n",
    "    polarities.append(blob.polarity)\n",
    "    subjectivities.append(blob.subjectivity)\n",
    "    if blob.polarity > 0:\n",
    "        positive_count += 1\n",
    "    elif blob.polarity < 0:\n",
    "        negative_count += 1\n",
    "    else:\n",
    "        neutral_count += 1\n",
    "\n",
    "print(f\"Positive Articles: {positive_count}\")\n",
    "print(f\"Negative Articles: {negative_count}\")\n",
    "print(f\"Neutral Articles: {neutral_count}\")\n",
    "\n",
    "# Visualize sentiment polarity and subjectivity\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x=polarities, y=subjectivities, alpha=0.6, color=\"purple\")\n",
    "plt.title(\"Sentiment Analysis of Articles\")\n",
    "plt.xlabel(\"Polarity\")\n",
    "plt.ylabel(\"Subjectivity\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Polarity distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(polarities, bins=20, kde=True, color=\"red\")\n",
    "plt.title(\"Distribution of Polarity\")\n",
    "plt.xlabel(\"Polarity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Subjectivity distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(subjectivities, bins=20, kde=True, color=\"blue\")\n",
    "plt.title(\"Distribution of Subjectivity\")\n",
    "plt.xlabel(\"Subjectivity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# === Question 6: Token Length Distribution (New Analysis) ===\n",
    "token_lengths = [len(word_tokenize(text)) for text in texts]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(token_lengths, bins=30, kde=True, color=\"orange\")\n",
    "plt.title(\"Distribution of Token Lengths\")\n",
    "plt.xlabel(\"Number of Tokens\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# === Question 7: Average Word Length (New Analysis) ===\n",
    "avg_word_lengths = [np.mean([len(word) for word in text.split()]) for text in texts]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(avg_word_lengths, bins=30, kde=True, color=\"green\")\n",
    "plt.title(\"Distribution of Average Word Lengths\")\n",
    "plt.xlabel(\"Average Word Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# === Question 8: Lexical Diversity (New Analysis) ===\n",
    "def compute_lexical_diversity(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return len(set(tokens)) / len(tokens)\n",
    "\n",
    "lexical_diversities = [compute_lexical_diversity(text) for text in texts]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(lexical_diversities, bins=30, kde=True, color=\"purple\")\n",
    "plt.title(\"Distribution of Lexical Diversity\")\n",
    "plt.xlabel(\"Lexical Diversity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
